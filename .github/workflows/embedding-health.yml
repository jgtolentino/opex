name: Embedding Health Monitoring

on:
  schedule:
    # Run daily at 3 AM UTC (after maintenance function runs at 2 AM)
    - cron: '0 3 * * *'

  workflow_dispatch:
    inputs:
      export_report:
        description: 'Export detailed report'
        required: false
        default: 'false'

jobs:
  health-check:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install psycopg2-binary tabulate

      - name: Run health audit
        id: audit
        env:
          POSTGRES_URL: ${{ secrets.POSTGRES_URL }}
        run: |
          python scripts/embedding_audit.py --health --coverage > audit_output.txt
          cat audit_output.txt

          # Export detailed report
          python scripts/embedding_audit.py --export audit_report.json

      - name: Check health thresholds
        id: check_thresholds
        env:
          POSTGRES_URL: ${{ secrets.POSTGRES_URL }}
        run: |
          # Extract metrics from report
          total=$(python -c "import json; data=json.load(open('audit_report.json')); print(data['health_metrics']['total_sources'])")
          failed=$(python -c "import json; data=json.load(open('audit_report.json')); print(data['health_metrics']['failed_sources'])")
          stale=$(python -c "import json; data=json.load(open('audit_report.json')); print(data['health_metrics']['stale_sources'])")
          coverage=$(python -c "import json; data=json.load(open('audit_report.json')); print(data['coverage']['coverage_percentage'])")

          echo "Total sources: $total"
          echo "Failed sources: $failed"
          echo "Stale sources: $stale"
          echo "Coverage: $coverage%"

          # Check thresholds
          ALERT_NEEDED=false
          ALERT_MESSAGE=""

          if [ "$failed" -ge 5 ]; then
            ALERT_NEEDED=true
            ALERT_MESSAGE="${ALERT_MESSAGE}\n‚ö†Ô∏è Critical: $failed sources permanently failed (threshold: 5)"
          fi

          if [ "$total" -gt 0 ]; then
            stale_pct=$(python -c "print(round(($stale / $total) * 100, 2))")
            if (( $(echo "$stale_pct > 20" | bc -l) )); then
              ALERT_NEEDED=true
              ALERT_MESSAGE="${ALERT_MESSAGE}\n‚ö†Ô∏è High staleness: $stale sources ($stale_pct%) not updated in 30 days"
            fi
          fi

          if (( $(echo "$coverage < 80" | bc -l) )); then
            ALERT_NEEDED=true
            ALERT_MESSAGE="${ALERT_MESSAGE}\n‚ö†Ô∏è Low coverage: ${coverage}% (threshold: 80%)"
          fi

          echo "alert_needed=$ALERT_NEEDED" >> $GITHUB_OUTPUT
          echo "alert_message<<EOF" >> $GITHUB_OUTPUT
          echo -e "$ALERT_MESSAGE" >> $GITHUB_OUTPUT
          echo "EOF" >> $GITHUB_OUTPUT

          echo "total_sources=$total" >> $GITHUB_OUTPUT
          echo "failed_sources=$failed" >> $GITHUB_OUTPUT
          echo "stale_sources=$stale" >> $GITHUB_OUTPUT
          echo "coverage=$coverage" >> $GITHUB_OUTPUT

      - name: Create GitHub issue for critical failures
        if: steps.check_thresholds.outputs.alert_needed == 'true'
        uses: actions/github-script@v7
        with:
          script: |
            const alertMessage = `${{ steps.check_thresholds.outputs.alert_message }}`;
            const totalSources = ${{ steps.check_thresholds.outputs.total_sources }};
            const failedSources = ${{ steps.check_thresholds.outputs.failed_sources }};
            const staleSources = ${{ steps.check_thresholds.outputs.stale_sources }};
            const coverage = ${{ steps.check_thresholds.outputs.coverage }};

            const issueBody = `
            ## üö® Embedding Health Alert

            ${alertMessage}

            ### Current Metrics
            - **Total Sources**: ${totalSources}
            - **Failed Sources**: ${failedSources}
            - **Stale Sources**: ${staleSources}
            - **Coverage**: ${coverage}%

            ### Recommended Actions
            1. Review failed sources: \`python scripts/embedding_audit.py --failed\`
            2. Check stale sources: \`python scripts/embedding_audit.py --stale\`
            3. Trigger manual embedding worker: POST to \`/functions/v1/embedding-worker\`
            4. Review error logs in Supabase Edge Functions

            ### Related Files
            - SQL Schema: \`packages/db/sql/embedding_sources.sql\`
            - Worker Function: \`supabase/functions/embedding-worker/index.ts\`
            - Maintenance Function: \`supabase/functions/embedding-maintenance/index.ts\`

            ---
            *Generated by GitHub Actions - Embedding Health Monitoring*
            *Timestamp: ${new Date().toISOString()}*
            `;

            // Check if issue already exists
            const issues = await github.rest.issues.listForRepo({
              owner: context.repo.owner,
              repo: context.repo.repo,
              state: 'open',
              labels: 'embedding-health'
            });

            if (issues.data.length === 0) {
              // Create new issue
              await github.rest.issues.create({
                owner: context.repo.owner,
                repo: context.repo.repo,
                title: `üö® Embedding Health Alert - ${new Date().toISOString().split('T')[0]}`,
                body: issueBody,
                labels: ['embedding-health', 'priority-high', 'automated']
              });
            } else {
              // Update existing issue
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: issues.data[0].number,
                body: issueBody
              });
            }

      - name: Send Slack notification
        if: always()
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
        run: |
          if [ -z "$SLACK_WEBHOOK_URL" ]; then
            echo "SLACK_WEBHOOK_URL not configured, skipping notification"
            exit 0
          fi

          ALERT_NEEDED="${{ steps.check_thresholds.outputs.alert_needed }}"
          TOTAL="${{ steps.check_thresholds.outputs.total_sources }}"
          FAILED="${{ steps.check_thresholds.outputs.failed_sources }}"
          STALE="${{ steps.check_thresholds.outputs.stale_sources }}"
          COVERAGE="${{ steps.check_thresholds.outputs.coverage }}"

          if [ "$ALERT_NEEDED" = "true" ]; then
            COLOR="#ff0000"
            EMOJI=":rotating_light:"
            TITLE="Embedding Health Alert"
            MESSAGE="${{ steps.check_thresholds.outputs.alert_message }}"
          else
            COLOR="#36a64f"
            EMOJI=":white_check_mark:"
            TITLE="Embedding Health Check Passed"
            MESSAGE="All systems healthy\n‚Ä¢ Coverage: ${COVERAGE}%\n‚Ä¢ Failed: ${FAILED}\n‚Ä¢ Stale: ${STALE}"
          fi

          curl -X POST "$SLACK_WEBHOOK_URL" \
            -H 'Content-Type: application/json' \
            -d "{
              \"text\": \"${EMOJI} ${TITLE}\",
              \"attachments\": [{
                \"color\": \"${COLOR}\",
                \"text\": \"${MESSAGE}\n\nTotal Sources: ${TOTAL}\",
                \"footer\": \"Finance RAG Health Monitoring\",
                \"ts\": $(date +%s)
              }]
            }"

      - name: Upload audit report
        if: github.event.inputs.export_report == 'true' || steps.check_thresholds.outputs.alert_needed == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: embedding-audit-report-${{ github.run_number }}
          path: |
            audit_output.txt
            audit_report.json
          retention-days: 30
